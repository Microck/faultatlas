---
phase: 01-foundation
plan: "03"
type: execute
wave: 3
depends_on: ["01-01", "01-02"]
files_modified:
  - src/models/__init__.py
  - src/models/failure.py
  - src/models/trace_record.py
  - src/core/failure_detector.py
  - src/storage/cosmos_client.py
  - src/storage/trace_store.py
  - src/scripts/run_and_capture.py
  - tests/test_trace_store_memory.py
autonomous: true
user_setup:
  - service: azure-cosmos-db
    why: "Persist failures + traces; enable retrieval by failure_id"
    env_vars:
      - name: COSMOS_ENDPOINT
        source: "Azure Portal -> Cosmos DB account -> Keys -> URI"
      - name: COSMOS_KEY
        source: "Azure Portal -> Cosmos DB account -> Keys -> PRIMARY KEY"
      - name: COSMOS_DATABASE
        source: "Choose a database name (e.g. indagine)"
      - name: COSMOS_CONTAINER_TRACES
        source: "Choose a container name (e.g. traces)"
must_haves:
  truths:
    - "Failure Detector emits a structured FailureEvent for all three subject failures"
    - "A trace record is persisted and retrievable by failure_id"
    - "Local dev can run end-to-end without Azure using an in-memory store"
    - "At least one tracing span is emitted during end-to-end runs (console locally; Azure Monitor when configured)"
  artifacts:
    - path: "src/core/failure_detector.py"
      provides: "Failure detection wrapper for subject runs"
    - path: "src/models/trace_record.py"
      provides: "Pinned minimum TraceRecord schema for Phase 2 analyzers (schema_version=1)"
    - path: "src/storage/trace_store.py"
      provides: "Persist/retrieve trace records by failure_id"
    - path: "src/scripts/run_and_capture.py"
      provides: "One-command demo of failure -> stored trace -> retrieval"
  key_links:
    - from: "src/core/failure_detector.py"
      to: "src/models/trace_record.py"
      via: "construct/validate TraceRecord before returning"
      pattern: "TraceRecord"
    - from: "src/core/failure_detector.py"
      to: "src/storage/trace_store.py"
      via: "store_trace(failure_id, trace)"
      pattern: "store_.*trace"
    - from: "src/scripts/run_and_capture.py"
      to: "src/core/tracing.py"
      via: "configure_tracing() at process start"
      pattern: "configure_tracing"

---

<objective>
Add failure detection and trace persistence so later phases can analyze real artifacts (failure events + traces) instead of ad-hoc prints.

Purpose: Create the durable inputs to the analysis pipeline.
Output: FailureEvent model, FailureDetector wrapper, Cosmos-backed TraceStore (with in-memory fallback).
</objective>

<execution_context>
@/home/ubuntu/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@.planning/phases/01-foundation/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Define FailureEvent model + failure detection wrapper</name>
  <files>
 src/models/__init__.py
 src/models/failure.py
 src/models/trace_record.py
 src/core/failure_detector.py
  </files>
  <action>
- Create `src/models/failure.py` (Pydantic) with:
  - `failure_id` (string, stable id), `subject`, `failure_type`, `timestamp`, `trace_id` (optional), `error` (string), and `metadata` (dict).
- Create `src/models/trace_record.py` (Pydantic) to pin a *minimum* trace record contract for downstream analyzers.
  - Make this a hard contract (no implicit fields): set Pydantic `extra='forbid'` on the model(s).
  - Include `schema_version: int = 1` and keep this minimum shape stable (these are the required fields analyzers can rely on):
    - `schema_version: int` (must be 1 for Phase 1)
    - `failure_id: str`
    - `subject: str`
    - `status: str` (must be one of: `failed|hallucinated|passed`)
    - `started_at: str` and `ended_at: str` (RFC3339 timestamps)
    - `steps: list[TraceStep]`
  - Each `TraceStep` must include (minimum):
    - `name: str` (human label)
    - `kind: str` (e.g. `tool_call|model_output|exception|validation_error`)
    - `input: dict|None`
    - `output: dict|None`
    - `error: str|None`
- Implement `src/core/failure_detector.py` with a single entrypoint `run_with_failure_detection(subject_name, fn, *, timeout_s)` that:
  - Runs a subject scenario function.
  - Classifies failures into: `exception`, `validation_error`, `timeout`, `hallucination_flag`.
  - Returns `(failure_event, trace_record)` where trace_record is a JSON-serializable structure (steps/tool calls/outputs/errors) that validates against `TraceRecord`.
  </action>
  <verify>
python3 -c "from src.core.failure_detector import run_with_failure_detection; from src.models.trace_record import TraceRecord; from src.subjects.booking_agent import run_booking_scenario; from src.subjects.search_agent import run_search_scenario; from src.subjects.summary_agent import run_summary_scenario; required={'schema_version','failure_id','subject','status','started_at','ended_at','steps'}; step_required={'name','kind','input','output','error'}; fe,tr=run_with_failure_detection('booking', run_booking_scenario, timeout_s=5); m=TraceRecord.model_validate(tr); assert required.issubset(set(m.model_dump().keys())); assert m.schema_version==1; assert m.subject=='booking'; assert m.status=='failed'; assert (not m.steps) or step_required.issubset(set(m.steps[0].model_dump().keys())); fe,tr=run_with_failure_detection('search', run_search_scenario, timeout_s=5); m=TraceRecord.model_validate(tr); assert required.issubset(set(m.model_dump().keys())); assert m.schema_version==1; assert m.subject=='search'; assert m.status=='failed'; assert (not m.steps) or step_required.issubset(set(m.steps[0].model_dump().keys())); fe,tr=run_with_failure_detection('summary', run_summary_scenario, timeout_s=5); m=TraceRecord.model_validate(tr); assert required.issubset(set(m.model_dump().keys())); assert m.schema_version==1; assert m.subject=='summary'; assert m.status=='hallucinated'; assert (not m.steps) or step_required.issubset(set(m.steps[0].model_dump().keys())); print('trace-record-contract-ok')"
  </verify>
  <done>
- All three subject scenarios produce a `FailureEvent` + trace_record (even hallucination)
- trace_record matches a pinned minimum schema (schema_version=1 + required fields + forbidden extras)
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement TraceStore with Cosmos backend + in-memory fallback</name>
  <files>
src/storage/cosmos_client.py
src/storage/trace_store.py
tests/test_trace_store_memory.py
  </files>
  <action>
- Implement `src/storage/trace_store.py` with an interface-like class `TraceStore`:
  - `store_trace(failure_event, trace_record) -> None`
  - `get_trace(failure_id) -> {failure_event, trace_record}`
- Implement a Cosmos backend in `src/storage/cosmos_client.py` and wire it into `TraceStore`.
- Add an in-memory implementation (either in the same file or separate small class) used by tests when Cosmos env vars are absent.
- Store trace documents keyed by `failure_id` and include a partition key (e.g. subject) so retrieval is constant-time.
- Add `tests/test_trace_store_memory.py` to verify store/get roundtrip for the in-memory implementation.
  </action>
  <verify>
python3 -m pytest -q
  </verify>
  <done>
- TraceStore supports `store_trace` + `get_trace`
- In-memory path is covered by tests
  </done>
</task>

<task type="auto">
  <name>Task 3: Add one end-to-end script: run subject -> detect failure -> persist -> retrieve</name>
  <files>
src/scripts/run_and_capture.py
  </files>
  <action>
- Create `src/scripts/run_and_capture.py` that:
  - Calls `configure_tracing()` once at process start.
  - Runs all three subject scenarios using `FailureDetector`.
  - Stores each trace using `TraceStore`.
  - Retrieves each by `failure_id` and prints a short JSON summary (failure_id, subject, failure_type, steps_count).
- Script must support `--store memory|cosmos` flag (default: memory) to allow running without Azure.
- Ensure at least one span is actually emitted during a run:
  - Either wrap each scenario run in a manual span (e.g., `run_subject:<name>`), or ensure FailureDetector creates a span around the subject execution.
  </action>
  <verify>
python3 -m src.scripts.run_and_capture --store memory
  </verify>
  <done>
- Script runs end-to-end locally and demonstrates persistence + retrieval
- Console span output is visible in local fallback mode (proof tracing is wired into a runnable entrypoint)
  </done>
</task>

</tasks>

<verification>
- `python3 -m src.scripts.run_and_capture --store memory` prints 3 stored+retrieved entries
- With Cosmos configured, `--store cosmos` persists and retrieves the same entries
</verification>

<success_criteria>
- Phase 1 success criteria are met: failure detection works and traces can be retrieved by `failure_id`
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md`
</output>
