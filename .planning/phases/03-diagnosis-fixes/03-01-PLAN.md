---
phase: 03-diagnosis-fixes
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - src/models/diagnosis.py
  - src/core/diagnosis_engine.py
  - tests/fixtures/findings/booking_findings.json
  - tests/fixtures/findings/context_overflow_findings.json
  - tests/fixtures/findings/coordination_findings.json
  - tests/fixtures/findings/reasoning_error_findings.json
  - tests/fixtures/findings/search_findings.json
  - tests/fixtures/findings/summary_findings.json
  - tests/test_diagnosis_engine.py
autonomous: true
must_haves:
  truths:
    - "Diagnosis Engine can emit every taxonomy type (all 6) using deterministic signals"
    - "Diagnosis output includes root_cause + confidence + explanation"
    - "Diagnosis output includes linkable references for similar past failures (IDs), not just a count"
  artifacts:
    - path: "src/core/diagnosis_engine.py"
      provides: "Deterministic diagnosis from analyzer findings"
    - path: "src/models/diagnosis.py"
      provides: "Diagnosis output contract"
  key_links:
    - from: "src/core/diagnosis_engine.py"
      to: "src/models/findings.py"
      via: "inputs are FindingsReport"
      pattern: "FindingsReport"

---

<objective>
Implement the Diagnosis Engine that converts analyzer findings into a root-cause diagnosis using the failure taxonomy.

Purpose: Provide the “why” behind a failure in a structured, testable way.
Output: Diagnosis models + DiagnosisEngine + tests.
</objective>

<execution_context>
@/home/ubuntu/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/02-analysis-pipeline/02-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add diagnosis models including failure taxonomy enum</name>
  <files>
src/models/diagnosis.py
  </files>
  <action>
- Create `src/models/diagnosis.py` (Pydantic) containing:
  - `FailureTaxonomy` enum with exactly: PROMPT_AMBIGUITY, TOOL_MISUSE, HALLUCINATION, CONTEXT_OVERFLOW, REASONING_ERROR, COORDINATION_FAILURE
  - `Diagnosis` model with: `root_cause` (FailureTaxonomy), `sub_type` (optional string), `confidence` (0.0-1.0 float), `explanation` (string), `affected_subjects` (list[str]), and `similar_past_failures` (int).
- Update the `Diagnosis` contract to satisfy DIAG-04 linkability:
  - Add `similar_past_failure_ids: list[str]` (default empty).
  - Keep `similar_past_failures` as a derived count for convenience (must match `len(similar_past_failure_ids)`).
- Keep it strict (validation errors should be obvious in tests).
  </action>
  <verify>
python3 -c "from src.models.diagnosis import FailureTaxonomy; print([t.value for t in FailureTaxonomy])"
  </verify>
  <done>
- Diagnosis model exists with taxonomy exactly matching the roadmap
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement DiagnosisEngine with deterministic rule-based classification</name>
  <files>
src/core/diagnosis_engine.py
tests/fixtures/findings/booking_findings.json
tests/fixtures/findings/context_overflow_findings.json
tests/fixtures/findings/coordination_findings.json
tests/fixtures/findings/reasoning_error_findings.json
tests/fixtures/findings/search_findings.json
tests/fixtures/findings/summary_findings.json
tests/test_diagnosis_engine.py
  </files>
  <action>
- Implement `src/core/diagnosis_engine.py` with `diagnose(findings_report) -> Diagnosis`.
- Classification rules (MVP, deterministic):
  - If ToolFinding contains schema mismatches or wrong-tool flags: root_cause=TOOL_MISUSE.
  - If trace metadata indicates `hallucinated=true`: root_cause=HALLUCINATION.
  - If trace indicates missing required info / multiple interpretations marker: root_cause=PROMPT_AMBIGUITY.
- Add deterministic signals to cover the remaining taxonomy types:
  - CONTEXT_OVERFLOW when the trace/trace-finding error contains a clear marker (prefer a fixture-stable token like `CONTEXT_OVERFLOW` or `CONTEXT_LENGTH_EXCEEDED`; also accept common substrings like "token limit" / "context length").
  - COORDINATION_FAILURE when the trace/trace-finding reasoning chain or error contains a clear marker (prefer a fixture-stable token like `COORDINATION_FAILURE` or phrases like "handoff" / "delegate" / "agent A" / "agent B").
  - REASONING_ERROR remains the fallback when no stronger signal matches.
- Confidence is a simple heuristic (e.g. 0.9 when strong signals exist, 0.6 otherwise).
- Add findings fixtures (JSON) that match the outputs from Phase 2 *and* prove all 6 taxonomy outcomes:
  - booking_findings.json -> TOOL_MISUSE
  - search_findings.json -> PROMPT_AMBIGUITY (or TOOL_MISUSE if tool signals are stronger; pick one and encode fixture signals accordingly)
  - summary_findings.json -> HALLUCINATION
  - context_overflow_findings.json -> CONTEXT_OVERFLOW (must include the stable marker string)
  - coordination_findings.json -> COORDINATION_FAILURE (must include the stable marker string)
  - reasoning_error_findings.json -> REASONING_ERROR explicitly (no other strong markers)
- Add `tests/test_diagnosis_engine.py` asserting:
  - Every taxonomy type is emitted by at least one fixture.
  - `similar_past_failures == len(similar_past_failure_ids)` and both are present on the output.
  </action>
  <verify>
python3 -m pytest -q
  </verify>
  <done>
- DiagnosisEngine passes tests and produces stable, explainable outputs
  </done>
</task>

</tasks>

<verification>
- `python3 -m pytest -q` passes
</verification>

<success_criteria>
- Phase 3 DIAG-01..DIAG-03 are satisfied with a deterministic, testable implementation
</success_criteria>

<output>
After completion, create `.planning/phases/03-diagnosis-fixes/03-01-SUMMARY.md`
</output>
