---
phase: 04-demo-submit
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - demo/scenario.md
  - demo/run_demo.py
  - demo/sample_output.md
autonomous: true
must_haves:
  truths:
    - "A scripted demo exists for the BookingAgent date-format failure"
    - "Demo can be run in a no-Azure fallback mode using fixtures"
  artifacts:
    - path: "demo/scenario.md"
      provides: "2-minute demo script and narration"
    - path: "demo/run_demo.py"
      provides: "One command to produce demo output"
    - path: "demo/sample_output.md"
      provides: "Backup output if Azure is unavailable"
  key_links:
    - from: "demo/run_demo.py"
      to: "src/core/autopsy_pipeline.py"
      via: "live mode runs end-to-end pipeline"
      pattern: "autopsy_pipeline|run_autopsy"
    - from: "demo/run_demo.py"
      to: "src/core/diagnosis_engine.py"
      via: "live mode diagnosis"
      pattern: "diagnose\("
    - from: "demo/run_demo.py"
      to: "src/core/fix_generator.py"
      via: "live mode fix proposals"
      pattern: "generate_fixes\("

---

<objective>
Create the demo script and a repeatable demo runner that produces the end-to-end output used in the video.

Purpose: Make the video recording reliable and fast.
Output: Demo script, demo runner, and sample output fallback.
</objective>

<execution_context>
@/home/ubuntu/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@/home/ubuntu/workspace/ai-dev-days-hackathon/plans/agent-autopsy.md
@.planning/phases/03-diagnosis-fixes/03-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write a time-boxed 2-minute demo scenario script</name>
  <files>
demo/scenario.md
  </files>
  <action>
- Create `demo/scenario.md` with:
  - A 2-minute timeline (0:00-2:00) and exact narration.
  - The on-screen actions (commands to run) aligned to timestamps.
  - The exact failure input: date in DD/MM/YYYY.
  - The outputs to highlight: failure detected, analyzer findings, diagnosis, fix diff.
  </action>
  <verify>
python3 -c "from pathlib import Path; print('lines', len(Path('demo/scenario.md').read_text().splitlines()))"
  </verify>
  <done>
- Demo script is complete and includes all required beats
  </done>
</task>

<task type="auto">
  <name>Task 2: Add a demo runner with a mock/fallback mode</name>
  <files>
demo/run_demo.py
demo/sample_output.md
  </files>
  <action>
- Implement `demo/run_demo.py` supporting:
  - `--mode live` to run the real, end-to-end pipeline by calling the actual modules (not `run_and_capture`):
    - Run the chosen subject scenario -> FailureDetector -> TraceStore.store_trace (memory by default).
    - Load the stored trace via `AutopsyPipeline` (TraceStore injection) and run analyzers via AutopsyController.
    - Diagnose via `DiagnosisEngine`.
    - Propose fixes via `FixGenerator`.
    - Print a coherent JSON (or Markdown) output with the same sections as mock mode.
  - Include `--store memory|cosmos` for `--mode live` (default: memory) so the demo can run without Azure.
  - `--mode mock` to print a pre-baked JSON output loaded from fixture files so the demo never blocks on Azure.
- Add `demo/sample_output.md` containing the expected output sections (failure event, findings, diagnosis, fix diff) used in `--mode mock`.
  </action>
  <verify>
python3 demo/run_demo.py --mode mock
python3 demo/run_demo.py --mode live --store memory
  </verify>
  <done>
- Mock demo prints a coherent end-to-end story in under 5 seconds
- Live demo exercises the actual pipeline modules end-to-end (FailureDetector -> TraceStore -> AutopsyPipeline -> DiagnosisEngine -> FixGenerator)
  </done>
</task>

</tasks>

<verification>
- `python3 demo/run_demo.py --mode mock` runs successfully
</verification>

<success_criteria>
- DEMO-01 foundation exists: a rehearsable script and a repeatable runner
</success_criteria>

<output>
After completion, create `.planning/phases/04-demo-submit/04-01-SUMMARY.md`
</output>
